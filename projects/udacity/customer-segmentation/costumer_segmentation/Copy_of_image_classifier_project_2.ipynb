{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of image_classifier_project_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Rvf4r1KvmZ0R",
        "colab_type": "code",
        "outputId": "3267fd4b-76b0-445d-e6b3-682b4964c052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cik31wNyp4HA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Developing an AI application\n",
        "\n",
        "Going forward, AI algorithms will be incorporated into more and more everyday applications. For example, you might want to include an image classifier in a smart phone app. To do this, you'd use a deep learning model trained on hundreds of thousands of images as part of the overall application architecture. A large part of software development in the future will be using these types of models as common parts of applications. \n",
        "\n",
        "In this project, you'll train an image classifier to recognize different species of flowers. You can imagine using something like this in a phone app that tells you the name of the flower your camera is looking at. In practice you'd train this classifier, then export it for use in your application. We'll be using [this dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) of 102 flower categories, you can see a few examples below. \n",
        "\n",
        "<img src='https://drive.google.com/open?id=1SCxL_JlKANLPwIFsGJF1QCx9Mc9FanoQ' width=500px> \n",
        "\n",
        "The project is broken down into multiple steps:\n",
        "\n",
        "* Load and preprocess the image dataset\n",
        "* Train the image classifier on your dataset\n",
        "* Use the trained classifier to predict image content\n",
        "\n",
        "We'll lead you through each part which you'll implement in Python.\n",
        "\n",
        "When you've completed this project, you'll have an application that can be trained on any set of labeled images. Here your network will be learning about flowers and end up as a command line application. But, what you do with your new skills depends on your imagination and effort in building a dataset. For example, imagine an app where you take a picture of a car, it tells you what the make and model is, then looks up information about it. Go build your own dataset and make something new.\n",
        "\n",
        "First up is importing the packages you'll need. It's good practice to keep all the imports at the beginning of your code. As you work through this notebook and find you need to import a package, make sure to add the import up here."
      ]
    },
    {
      "metadata": {
        "id": "vnc0hLiVssER",
        "colab_type": "code",
        "outputId": "af4ea913-1e90-407a-b62b-af64a4658d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls '/content/gdrive/My Drive/DataScience/p2_image_classifier/flowers'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test  train  valid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yIjt_VDWyOnO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " #Imports\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch import nn as nn\n",
        "from torch import optim as optim\n",
        "from torch import cuda as cuda\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary as summary\n",
        "from torchvision import datasets, models, transforms\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['font.size'] = 14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LlYKF0jcyeaz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the data\n",
        "\n",
        "Here you'll use `torchvision` to load the data ([documentation](http://pytorch.org/docs/0.3.0/torchvision/index.html)). The data should be included alongside this notebook, otherwise you can [download it here](https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz). The dataset is split into three parts, training, validation, and testing. For the training, you'll want to apply transformations such as random scaling, cropping, and flipping. This will help the network generalize leading to better performance. You'll also need to make sure the input data is resized to 224x224 pixels as required by the pre-trained networks.\n",
        "\n",
        "The validation and testing sets are used to measure the model's performance on data it hasn't seen yet. For this you don't want any scaling or rotation transformations, but you'll need to resize then crop the images to the appropriate size.\n",
        "\n",
        "The pre-trained networks you'll use were trained on the ImageNet dataset where each color channel was normalized separately. For all three sets you'll need to normalize the means and standard deviations of the images to what the network expects. For the means, it's `[0.485, 0.456, 0.406]` and for the standard deviations `[0.229, 0.224, 0.225]`, calculated from the ImageNet images.  These values will shift each color channel to be centered at 0 and range from -1 to 1.\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "yTVR5Nj5yqJB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define some utility classes\n",
        "batch_size = 10\n",
        "data_dir = '/content/gdrive/My Drive/DataScience/p2_image_classifier/flowers'\n",
        "train_dir = data_dir + '/train/'\n",
        "validation_dir = data_dir + '/valid/'\n",
        "test_dir = data_dir + '/test/'\n",
        "\n",
        "channels_mean_sequence = [0.485, 0.456, 0.406]\n",
        "channels_standard_deviation_sequence = [0.229, 0.224, 0.225]\n",
        "dimensions = [224, 224]\n",
        "extra_dimensions = [256, 256]\n",
        "\n",
        "\n",
        "images_dataset_names = ['training_images','test_images', 'validation_images']\n",
        "image_directories = {images_dataset_names[0]:train_dir, \n",
        "                     images_dataset_names[1]:test_dir, images_dataset_names[2]:validation_dir}\n",
        "\n",
        "class Analyzer:\n",
        "    def __init__(self):\n",
        "        self.training_losses = []\n",
        "        self.validation_losses = []\n",
        "        self.training_accuracy = []\n",
        "        self.validation_accuracy = []\n",
        "        \n",
        "    def addlosses(self, losses, work_flow_phase, epoch):\n",
        "        if work_flow_phase == 'training_images':\n",
        "           tripple = (work_flow_phase, epoch, losses)\n",
        "           self.training_losses.append(tripple)\n",
        "        elif work_flow_phase == 'validation_images':\n",
        "           tripple = (work_flow_phase, epoch, losses)\n",
        "           self.validation_losses.append(tripple)\n",
        "            \n",
        "    def addaccuracy(self, accuracy, work_flow_phase, epoch):\n",
        "        if work_flow_phase == 'training_images':\n",
        "           tripple = (work_flow_phase, epoch, accuracy)\n",
        "           self.training_accuracy.append(tripple)\n",
        "        elif work_flow_phase == 'validation_images':\n",
        "           tripple = (work_flow_phase, epoch, accuracy)\n",
        "           self.validation_accuracy.append(tripple)\n",
        "            \n",
        "    def plotlosses(self):\n",
        "        training_losses = []\n",
        "        validation_losses = []\n",
        "        epochs = []\n",
        "        first = self.training_losses[0]\n",
        "        training_phase = first[0]\n",
        "        for tripple in self.training_losses:\n",
        "            epochs.append(tripple[1])\n",
        "            training_losses.append(tripple[2])\n",
        "        \n",
        "        first = self.validation_losses[0]\n",
        "        validation_phase = first[0]\n",
        "        for tripple in self.validation_losses:\n",
        "            validation_losses.append(tripple[2])\n",
        "        \n",
        "        plt.plot(epochs, training_losses , 'r--', label=training_phase)\n",
        "        plt.plot(epochs, validation_losses, 'g^', label=validation_phase)\n",
        "        plt.show()\n",
        "    \n",
        "    def plotaccuracy(self):\n",
        "        training_accuracies = []\n",
        "        validation_accuracies = []\n",
        "        epochs = []\n",
        "        first = self.training_accuracy[0]\n",
        "        training_phase = first[0]\n",
        "        for tripple in self.training_accuracy:\n",
        "            epochs.append(tripple[1])\n",
        "            training_accuracies.append(tripple[2])\n",
        "        \n",
        "        first = self.validation_accuracy[0]\n",
        "        validation_phase = first[0]\n",
        "        for tripple in self.validation_accuracy:\n",
        "            validation_accuracies.append(tripple[2])\n",
        "        \n",
        "        plt.plot(epochs, training_accuracies, label=training_phase)\n",
        "        plt.plot(epochs, validation_accuracies, 'bs', label=validation_phase)\n",
        "        plt.show()\n",
        "\n",
        "class ImageConverter:\n",
        "    def __init__(self, tensor):\n",
        "        self.tensor = tensor\n",
        "    def convert(self, tensor):\n",
        "         converted = self.tensor.cpu().clone().detach().numpy()\n",
        "         converted = image.transpose(1, 2, 0)\n",
        "         converted = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
        "         converted = image.clip(0, 1)\n",
        "         return image\n",
        "    \n",
        "class Check_point_manager:\n",
        "    def __init__(self, modelName, best_model_wts, linear_classifier,optimizer, model):\n",
        "        self.saved_state = {'arch_name':modelName,\n",
        "                            'best_model_wts':best_model_wts,\n",
        "                            'linear_classifier':linear_classifier,\n",
        "                            'optimizer_state':optimizer.,\n",
        "                            'class_to_idx': model.class_to_idx,\n",
        "                            'idx_to_class': model.idx_to_class\n",
        "                           }\n",
        "        \n",
        "    def save_state(self, path):\n",
        "        torch.save(self.saved_state, path)\n",
        "    \n",
        "    def load_state(self, path):\n",
        "        self.elf.saved_state = torch.load(path)\n",
        "    \n",
        "class NeuralNetworkModelBuilder:\n",
        "    def __init__(self, modelName):\n",
        "        self.n_outputs = 102\n",
        "        self.modelName = modelName\n",
        "        \n",
        "        if modelName != None and modelName == \"AlexNet\":\n",
        "            self.model = models.alexnet(pretrained=True)\n",
        "        elif modelName != None and modelName == \"VGG16\":\n",
        "            self.model = models.vgg16(pretrained=True)\n",
        "        elif modelName != None and modelName == \"VGG19\":\n",
        "            self.model = models.vgg19(pretrained=True)\n",
        "        elif modelName != None and modelName == \"ResNet18\":\n",
        "            self.model = models.resnet18(pretrained=True)\n",
        "        else:\n",
        "            self.model = models.vgg19(pretrained=True)\n",
        "            \n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad_(False)\n",
        "    \n",
        "    def getPretrainedCNNModel(self):\n",
        "        return self.model\n",
        "    \n",
        "    def buildAndAddFullyConnectedLayer(self, modelName):\n",
        "        if modelName != None and modelName == \"VGG16\":\n",
        "            number_of_inputs = self.model.classifier[0].in_features\n",
        "            fclayer1 = nn.Linear(number_of_inputs, 4096)\n",
        "            relu = nn.ReLU()\n",
        "            fclayer2 = nn.Linear(4096, self.n_outputs)\n",
        "            output = nn.LogSoftmax(dim=1)\n",
        "            dropout = nn.Dropout(0.3)\n",
        "            classifier = nn.Sequential(\n",
        "                                       fclayer1 ,\n",
        "                                       relu,\n",
        "                                       #dropout,\n",
        "                                       fclayer2,\n",
        "                                       output\n",
        "                                       )\n",
        "            #self.model.classifier[6] = classifier\n",
        "            self.model.classifier = classifier\n",
        "        elif modelName != None and modelName == \"VGG19\":\n",
        "            number_of_inputs = self.model.classifier[0].in_features\n",
        "            fclayer1 = nn.Linear(number_of_inputs, 4096)\n",
        "            relu = nn.ReLU()\n",
        "            fclayer2 = nn.Linear(4096, self.n_outputs)\n",
        "            output = nn.LogSoftmax(dim=1)\n",
        "            dropout = nn.Dropout(0.3)\n",
        "            classifier = nn.Sequential(\n",
        "                                       fclayer1 ,\n",
        "                                       relu,\n",
        "                                       #dropout,\n",
        "                                       fclayer2,\n",
        "                                       output\n",
        "                                       )\n",
        "            #self.model.classifier[6] = classifier\n",
        "            self.model.classifier = classifier\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6AizQyxby7F4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataSets:\n",
        "    def __init__(self, data_transforms):\n",
        "        self.datasets = {idx: datasets.ImageFolder(image_directories[idx],   \n",
        "                transform=data_transforms[idx]) \n",
        "                for idx in images_dataset_names}\n",
        "    \n",
        "    def get_data_sets(self):\n",
        "        return self.datasets\n",
        "    \n",
        "    def get_training_dataset(self):\n",
        "        key = images_dataset_names[0]\n",
        "        return self.datasets[key]\n",
        "    \n",
        "    def get_validation_dataset(self):\n",
        "        key = images_dataset_names[1]\n",
        "        return self.datasets[key]\n",
        "    \n",
        "    def get_test_dataset(self):\n",
        "        key = images_dataset_names[2]\n",
        "        return self.datasets[key]\n",
        "\n",
        "class Transforms:\n",
        "    def __init__(self):\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        self.data_transforms = {\n",
        "                                images_dataset_names[0]: transforms.Compose([\n",
        "                                                     transforms.RandomRotation(25),\n",
        "                                                     transforms.RandomResizedCrop(224),\n",
        "                                                     #transforms.Resize(extra_dimensions), \n",
        "                                                     #transforms.CenterCrop(dimensions), \n",
        "                                                     transforms.RandomHorizontalFlip(),\n",
        "                                                     transforms.ToTensor(),\n",
        "                                                     transforms.Normalize(channels_mean_sequence, \n",
        "                                                                          channels_standard_deviation_sequence)\n",
        "                                ]),\n",
        "    \n",
        "                                images_dataset_names[1]: transforms.Compose([\n",
        "                                                     transforms.Resize(extra_dimensions), \n",
        "                                                     transforms.CenterCrop(dimensions), \n",
        "                                                     transforms.ToTensor(),\n",
        "                                                     transforms.Normalize(channels_mean_sequence, \n",
        "                                                                          channels_standard_deviation_sequence)\n",
        "                                ]),\n",
        "    \n",
        "                                images_dataset_names[2]: transforms.Compose([\n",
        "                                                     transforms.Resize(extra_dimensions),\n",
        "                                                     transforms.CenterCrop(dimensions), \n",
        "                                                     transforms.ToTensor(),\n",
        "                                                     transforms.Normalize(channels_mean_sequence, \n",
        "                                                                          channels_standard_deviation_sequence)\n",
        "                                ])\n",
        "                               }\n",
        "    def get_transforms(self):\n",
        "        return self.data_transforms\n",
        "    \n",
        "    def get_transforms_for_training_data(self):\n",
        "        key = images_dataset_names[0]\n",
        "        return self.data_transforms[key]\n",
        "    \n",
        "    \n",
        "    def get_transforms_for_validation_data(self):\n",
        "        key = images_dataset_names[1]\n",
        "        return self.data_transforms[key]\n",
        "    \n",
        "    \n",
        "    def get_transforms_for_test_data(self):\n",
        "        key = images_dataset_names[2]\n",
        "        return self.data_transforms[key]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iljfuOhg0wjM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define variables and transforms for the training, validation, and testing sets\n",
        "transformsLoc = Transforms()\n",
        "data_transforms = transformsLoc.get_transforms()\n",
        "\n",
        "image_datasets = DataSets(data_transforms).get_data_sets()\n",
        "\n",
        "#training_dataset = datasets.ImageFolder(train_dir, transform=transformsLoc.get_transforms_for_training_data())\n",
        "#validation_dataset = datasets.ImageFolder(validation_dir, transform=transformsLoc.get_transforms_for_validation_data())\n",
        "\n",
        "#training_loader = torch.utils.data.DataLoader(training_dataset, batch_size=20, shuffle=True)\n",
        "#validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size = 20, shuffle=False)\n",
        "#print(len(training_dataset))\n",
        "#print(len(validation_dataset))\n",
        "\n",
        "#image_datasets = {idx: datasets.ImageFolder(image_directories[idx],   \n",
        "#                 transform=data_transforms[idx]) \n",
        "#                 for idx in images_dataset_names}\n",
        "#print(type(image_datasets))\n",
        "#print(image_datasets)\n",
        "\n",
        "#test_data_transforms = transformsLoc.get_transforms_for_test_data()\n",
        "#print(test_data_transforms)\n",
        "\n",
        "# Using the image datasets and the transforms, define the dataloaders\n",
        "\n",
        "dataloaders = {ix: torch.utils.data.DataLoader(image_datasets[ix], \n",
        "               batch_size, shuffle=True) \n",
        "               for ix in images_dataset_names}\n",
        "\n",
        "#print(dataloaders)\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) \n",
        "                 for x in images_dataset_names}\n",
        "\n",
        "#print(dataset_sizes['training_images'])\n",
        "class_names = image_datasets['training_images'].classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_5Ne5qc3coz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/My Drive/DataScience/p2_image_classifier/cat_to_name.json', 'r') as f:\n",
        "    cat_to_name = json.load(f)\n",
        "\n",
        "#keys = cat_to_name.keys()\n",
        "#print(sorted(keys)) #Just Checking\n",
        "#print(len(cat_to_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DkUG-lRIFOGi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self, model):\n",
        "        self.batch_size = batch_size\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(self.device)\n",
        "        self.model = model\n",
        "        #self.dataloaders = dataloaders\n",
        "        #self.val_dataloader = val_dataloader\n",
        "        #self.train_dataloader = train_dataloader\n",
        "        \n",
        "        self.loss_function = nn.NLLLoss()\n",
        "        self.optimizer = optim.Adam(self.model.classifier.parameters(), lr = 0.0001)\n",
        "        self.epochs = 20\n",
        "        self.phases = ['training_images','validation_images']\n",
        "        self.epoch_loss = 0.0\n",
        "        self.epoch_corrects = 0\n",
        "        self.best_acc = 0.0\n",
        "        self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=4, gamma=0.1)\n",
        "        self.current_loss = 0.0\n",
        "        self.current_currects = 0\n",
        "        self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "        self.analyzer = Analyzer()\n",
        "        \n",
        "        #print(dataloaders.keys())\n",
        "        #print(dataloaders['test_images'])\n",
        "        #itr = iter(self.dataloaders['test_images'])\n",
        "        #x, y = itr.next()\n",
        "        #print(x.to(self.device))\n",
        "    def trainModel(self):\n",
        "        since = time.time()\n",
        "        #dataloader = self.train_dataloader \n",
        "        #best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "        #best_acc = 0.0\n",
        "        for epoch in range(self.epochs):\n",
        "            self.epoch_loss = 0.0\n",
        "            self.epoch_corrects = 0\n",
        "            print('Epoch {}/{}'.format(epoch, self.epochs - 1))\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            self.train_in_one_epoch(epoch)              \n",
        "                \n",
        "            #print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "              time_elapsed // 60, time_elapsed % 60))\n",
        "        print('Best val Acc: {:4f}'.format(self.best_acc))\n",
        "\n",
        "        # load best model weights\n",
        "        self.model.load_state_dict(self.best_model_wts)\n",
        "        \n",
        "        self.analyzer.plotlosses()\n",
        "        self.analyzer.plotaccuracy()\n",
        "        return\n",
        "    \n",
        "    def train_in_one_epoch(self, epoch):\n",
        "      \n",
        "        for phase in self.phases:\n",
        "            self.current_loss = 0.0\n",
        "            self.current_currects = 0\n",
        "            \n",
        "            if phase == 'training_images':\n",
        "               #dataLoader = self.train_dataloader\n",
        "               self.scheduler.step()\n",
        "               self.model.train()  # Set model to training mode\n",
        "            else:\n",
        "               self.model.eval()   # Set model to evaluate mode\n",
        "               #dataLoader = self.val_dataloader\n",
        "                \n",
        "               # Iterate over data.\n",
        "               #print(dataloaders[phase])\n",
        "               #loaders = self.dataloaders\n",
        "               #dataiter = iter(dataloaders[phase])\n",
        "               #nextpoint = dataiter.next()\n",
        "                 \n",
        "            for datapair in dataloaders[phase]:\n",
        "                data, targets = datapair\n",
        "                #while nextpoint != None:\n",
        "                #inputs, labels = nextpoint\n",
        "                #print(inputs)\n",
        "                #print(labels)\n",
        "                data = data.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'training_images'):\n",
        "                     outputs = self.model(data)\n",
        "                     _, predictions = torch.max(outputs.data, dim=1)\n",
        "                     self.loss = self.loss_function(outputs, targets)\n",
        "\n",
        "                     # backward + optimize only if in training phase\n",
        "                     if phase == 'training_images':\n",
        "                        self.loss.backward()\n",
        "                        self.optimizer.step()\n",
        "\n",
        "                \n",
        "            \n",
        "                self.current_loss += self.loss.item() * data.size(0)\n",
        "                self.current_currects += torch.sum(predictions == targets.data)\n",
        "                   \n",
        "            self.epoch_loss = self.current_loss / len(dataloaders[phase].dataset)\n",
        "            self.epoch_corrects = self.current_currects.double() / len(dataloaders[phase].dataset) #dataset_sizes[phase]\n",
        "            \n",
        "            self.analyzer.addlosses(self.epoch_loss, phase, epoch + 1)\n",
        "            self.analyzer.addaccuracy(self.epoch_corrects, phase, epoch + 1)\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                      phase, self.epoch_loss, self.epoch_corrects))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'validation_images' and self.epoch_corrects > self.best_acc:\n",
        "               self.best_acc = self.epoch_corrects\n",
        "               self.best_model_wts = copy.deepcopy(self.model.state_dict())\n",
        "               #self.model.load_state_dict(self.best_model_wts)\n",
        "        return\n",
        "    \n",
        "            self.loss_function = loss_function\n",
        "    def validate_model(self):\n",
        "        phase = 'test_images'\n",
        "        self.model.eval()\n",
        "        current_loss = 0.0\n",
        "        current_corrects = 0\n",
        "        dataloader = dataloaders['test_images']\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(self.device)\n",
        "            targets = targets.to(self.device)\n",
        "            with torch.set_grad_enabled(False):\n",
        "                outputs = self.model(data)\n",
        "                _, predictions = torch.max(outputs, dim=1)\n",
        "                loss = self.loss_function(outputs, targets)\n",
        "            current_loss += loss.item() * data.size(0)\n",
        "            current_corrects += torch.sum(predictions == targets.data)\n",
        "        epoch_loss = current_loss / dataset_sizes['test_images']\n",
        "        epoch_corrects = current_corrects.double() / dataset_sizes['test_images']   \n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                      phase, epoch_loss, epoch_corrects))\n",
        "        return\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VKUVSCA5DSCu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ImageClassifier:\n",
        "    def __init__(self, model, loss_function, device):\n",
        "        self. model = model\n",
        "        self.device = device\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f2CZ0jt7rISb",
        "colab_type": "code",
        "outputId": "0d5d9d27-ca3e-40f2-e161-349e54b6de83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        }
      },
      "cell_type": "code",
      "source": [
        "cnnModelBuilder = NeuralNetworkModelBuilder('VGG19')\n",
        "cnnModelBuilder.buildAndAddFullyConnectedLayer(cnnModelBuilder.modelName)\n",
        "cnnModelBuilder.model.class_to_idx = dataloaders[phase]['training_images'].class_to_idx\n",
        "cnnModelBuilder.model.idx_to_class = {\n",
        "    idx: class_names\n",
        "    for class_names, idx in model.class_to_idx.items()\n",
        "}\n",
        "\n",
        "print(cnnModelBuilder.model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace)\n",
            "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace)\n",
            "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace)\n",
            "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (24): ReLU(inplace)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): ReLU(inplace)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): ReLU(inplace)\n",
            "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (33): ReLU(inplace)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): ReLU(inplace)\n",
            "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=4096, out_features=102, bias=True)\n",
            "    (3): LogSoftmax()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NS9cYtYqOyLL",
        "colab_type": "code",
        "outputId": "fb17079b-fded-4f20-f481-de9801c49c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2069
        }
      },
      "cell_type": "code",
      "source": [
        "modelTrainer = ModelTrainer(cnnModelBuilder.model)\n",
        "modelTrainer.trainModel()\n",
        "modelTrainer.validate_model()\n",
        "\n",
        "#imageClassifier = ImageClassifier(modelTrainer.model, modelTrainer.device, modelTrainer.loss_function) \n",
        "#imageClassifier.classify()\n",
        "\n",
        "#imageClassifier = ImageClassifier(modelTrainer.model, modelTrainer.loss_function, modelTrainer.device) \n",
        "#imageClassifier.classify()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/14\n",
            "----------\n",
            "training_images Loss: 1.9856 Acc: 0.5302\n",
            "validation_images Loss: 0.7880 Acc: 0.7836\n",
            "Epoch 1/14\n",
            "----------\n",
            "training_images Loss: 0.9387 Acc: 0.7402\n",
            "validation_images Loss: 0.6316 Acc: 0.8203\n",
            "Epoch 2/14\n",
            "----------\n",
            "training_images Loss: 0.7159 Acc: 0.8025\n",
            "validation_images Loss: 0.5835 Acc: 0.8362\n",
            "Epoch 3/14\n",
            "----------\n",
            "training_images Loss: 0.6390 Acc: 0.8188\n",
            "validation_images Loss: 0.4882 Acc: 0.8606\n",
            "Epoch 4/14\n",
            "----------\n",
            "training_images Loss: 0.4685 Acc: 0.8698\n",
            "validation_images Loss: 0.3567 Acc: 0.9034\n",
            "Epoch 5/14\n",
            "----------\n",
            "training_images Loss: 0.4236 Acc: 0.8832\n",
            "validation_images Loss: 0.3463 Acc: 0.9046\n",
            "Epoch 6/14\n",
            "----------\n",
            "training_images Loss: 0.3759 Acc: 0.8987\n",
            "validation_images Loss: 0.3325 Acc: 0.9059\n",
            "Epoch 7/14\n",
            "----------\n",
            "training_images Loss: 0.3705 Acc: 0.8945\n",
            "validation_images Loss: 0.3216 Acc: 0.9095\n",
            "Epoch 8/14\n",
            "----------\n",
            "training_images Loss: 0.3548 Acc: 0.8993\n",
            "validation_images Loss: 0.3193 Acc: 0.9108\n",
            "Epoch 9/14\n",
            "----------\n",
            "training_images Loss: 0.3494 Acc: 0.9035\n",
            "validation_images Loss: 0.3173 Acc: 0.9108\n",
            "Epoch 10/14\n",
            "----------\n",
            "training_images Loss: 0.3457 Acc: 0.9045\n",
            "validation_images Loss: 0.3171 Acc: 0.9108\n",
            "Epoch 11/14\n",
            "----------\n",
            "training_images Loss: 0.3633 Acc: 0.9031\n",
            "validation_images Loss: 0.3186 Acc: 0.9083\n",
            "Epoch 12/14\n",
            "----------\n",
            "training_images Loss: 0.3359 Acc: 0.9075\n",
            "validation_images Loss: 0.3185 Acc: 0.9083\n",
            "Epoch 13/14\n",
            "----------\n",
            "training_images Loss: 0.3348 Acc: 0.9103\n",
            "validation_images Loss: 0.3183 Acc: 0.9083\n",
            "Epoch 14/14\n",
            "----------\n",
            "training_images Loss: 0.3281 Acc: 0.9092\n",
            "validation_images Loss: 0.3179 Acc: 0.9083\n",
            "Training complete in 92m 59s\n",
            "Best val Acc: 0.910758\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLyISIf8f/I9++N+nDLsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-efc85419c2f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#modelTrainer = ModelTrainer(cnnModelBuilder.model, validation_loader, training_loader)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodelTrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnnModelBuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodelTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#imageClassifier = ImageClassifier(modelTrainer.model, modelTrainer.device, modelTrainer.loss_function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-1b6c6b3c6384>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotlosses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotaccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-eae2d15b5406>\u001b[0m in \u001b[0;36mplotaccuracy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_phase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_phase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'validation_accuracy' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEQkQhb6ISID8Dyc41/he/ZLxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}